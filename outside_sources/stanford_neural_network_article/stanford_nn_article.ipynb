{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "based on [stanford lectures on neural networks](https://cs231n.github.io/neural-networks-1/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice use of np.sum instead of summing with a for loop.\n",
    "This is probably much quicker."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function\n",
    "    return firing_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On sigmoid as activation function and some of its flaws:\n",
    "- Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.\n",
    "\n",
    "- Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. x>0 elementwise in f=wTx+b)), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TLDR on activation function:\n",
    "- TLDR: “What neuron type should I use?” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. **Never use sigmoid**. Try tanh, but expect it to work worse than ReLU/Maxout."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![title](./stan_nn1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Sizing neural networks.\n",
    "The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture:\n",
    "\n",
    "- The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.\n",
    "- The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Forward passing of 3-layer neural network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (3,1) not aligned: 1 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [11], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;241m1.0\u001B[39m\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mx)) \u001B[38;5;66;03m# activation function (use sigmoid)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# random input vector of three numbers (3x1)\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m h1 \u001B[38;5;241m=\u001B[39m f(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m b1) \u001B[38;5;66;03m# calculate first hidden layer activations (4x1)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m h2 \u001B[38;5;241m=\u001B[39m f(np\u001B[38;5;241m.\u001B[39mdot(W2, h1) \u001B[38;5;241m+\u001B[39m b2) \u001B[38;5;66;03m# calculate second hidden layer activations (4x1)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m out \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(W3, h2) \u001B[38;5;241m+\u001B[39m b3\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (1,) and (3,1) not aligned: 1 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "W1,W2,W3 = np.random.random(1), np.random.random(1), np.random.random(1)\n",
    "b1,b2,b3 = 1,1,1\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of three numbers (3x1)\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}